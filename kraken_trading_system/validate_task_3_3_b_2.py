#!/usr/bin/env python3
"""
Task 3.3.B.2 Validation Script: Real-time Fill Analytics Engine

This script validates the implementation of the real-time analytics engine
and tests its core functionality to ensure Task 3.3.B.2 is complete.

Usage: python3 validate_task_3_3_b_2.py
"""

import asyncio
import sys
from pathlib import Path
from datetime import datetime, timedelta
from decimal import Decimal
from typing import Dict, Any

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_imports():
    """Test that all required modules can be imported."""
    print("ğŸ”¬ TESTING IMPORTS")
    print("=" * 50)

    try:
        from trading_systems.exchanges.kraken.realtime_analytics import (
            RealTimeAnalyticsEngine, RealTimePnL, ExecutionMetrics, RiskAlert, AlertLevel
        )
        print("âœ… RealTimeAnalyticsEngine imports successful")
        return True, RealTimeAnalyticsEngine
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        print("Checking if implementation file exists...")

        analytics_file = Path("src/trading_systems/exchanges/kraken/realtime_analytics.py")
        if analytics_file.exists():
            print(f"âœ… Found analytics file: {analytics_file}")
            print(f"ğŸ“Š File size: {analytics_file.stat().st_size} bytes")
        else:
            print(f"âŒ Analytics file not found: {analytics_file}")
        return False, None

def create_mock_fill(trade_id: str, order_id: str, volume: str, price: str, side: str = "buy", fee: str = "0"):
    """Create a mock TradeFill for testing."""
    from decimal import Decimal
    from datetime import datetime

    # Complete mock object that mimics TradeFill structure with all required attributes
    class MockTradeFill:
        def __init__(self, trade_id, order_id, volume, price, side, fee):
            self.trade_id = trade_id
            self.order_id = order_id
            self.volume = Decimal(volume)
            self.price = Decimal(price)
            self.side = side
            self.fee = Decimal(fee)
            self.cost = self.volume * self.price
            self.timestamp = datetime.now()
            self.pair = "XBT/USD"
            self.fill_type = "taker"
            self.fill_quality = "fair"

            # Additional attributes required by RealTimeAnalyticsEngine
            self.slippage = Decimal("0.001")  # Small positive slippage
            self.price_improvement = Decimal("0.002")  # Small price improvement
            self.execution_time = datetime.now()
            self.market_price = self.price  # Current market price
            self.vwap = self.price  # Volume weighted average price
            self.liquidity_consumed = Decimal("0.1")  # Liquidity impact
            self.implementation_shortfall = Decimal("0.0005")

            # Market context
            self.market_conditions = {
                'spread': Decimal("0.5"),
                'volatility': Decimal("0.02"),
                'volume': Decimal("100.0")
            }

    return MockTradeFill(trade_id, order_id, volume, price, side, fee)

async def test_basic_functionality(AnalyticsEngine):
    """Test basic analytics engine functionality."""
    print("\nğŸ”¬ TESTING BASIC ANALYTICS FUNCTIONALITY")
    print("=" * 60)

    try:
        # Initialize engine
        engine = AnalyticsEngine("TestAnalyticsEngine")

        # Test 1: Initial state
        print("\n1ï¸âƒ£ Testing Initial State...")
        assert engine.pnl.total_pnl == Decimal('0')
        assert engine.pnl.total_trades == 0
        assert len(engine.fill_history) == 0
        print("   âœ… Initial state correct")

        # Test 2: Process single fill
        print("\n2ï¸âƒ£ Testing Single Fill Processing...")
        fill = create_mock_fill("TEST_001", "ORDER_001", "1.0", "50000.00", "buy", fee="5.00")
        await engine.process_fill(fill)

        assert len(engine.fill_history) == 1
        assert engine.pnl.total_trades == 1
        assert engine.pnl.total_volume_traded == Decimal('1.0')
        assert engine.pnl.total_fees == Decimal('5.0')
        print("   âœ… Single fill processing works")

        # Test 3: Multiple fills
        print("\n3ï¸âƒ£ Testing Multiple Fills...")
        fills = [
            create_mock_fill("TEST_002", "ORDER_001", "0.5", "50100.00", "buy", fee="2.50"),
            create_mock_fill("TEST_003", "ORDER_002", "0.3", "49900.00", "sell", fee="1.50"),
        ]

        for fill in fills:
            await engine.process_fill(fill)

        assert len(engine.fill_history) == 3
        assert engine.pnl.total_trades == 3
        assert engine.pnl.total_volume_traded == Decimal('1.8')
        print("   âœ… Multiple fills processing works")

        # Test 4: Real-time dashboard
        print("\n4ï¸âƒ£ Testing Real-time Dashboard...")
        dashboard = engine.get_real_time_dashboard()

        assert 'pnl_summary' in dashboard
        assert 'trading_stats' in dashboard
        assert 'execution_quality' in dashboard
        assert dashboard['trading_stats']['total_trades'] == 3
        print("   âœ… Dashboard generation works")

        # Test 5: Performance report
        print("\n5ï¸âƒ£ Testing Performance Report...")
        report = engine.get_performance_report(timedelta(hours=1))

        assert 'summary_statistics' in report
        assert report['summary_statistics']['total_fills'] == 3
        assert 'quality_analysis' in report
        print("   âœ… Performance report generation works")

        print("\nğŸ‰ ALL BASIC TESTS PASSED!")
        return True

    except Exception as e:
        print(f"\nâŒ Basic analytics test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_risk_alert_system(AnalyticsEngine):
    """Test risk alert system functionality."""
    print("\nğŸš¨ TESTING RISK ALERT SYSTEM")
    print("=" * 60)

    try:
        engine = AnalyticsEngine("RiskTestEngine")

        # Test 1: Alert configuration
        print("\n1ï¸âƒ£ Testing Alert Configuration...")
        assert 'max_drawdown_pct' in engine.risk_thresholds
        assert 'max_position_size' in engine.risk_thresholds
        assert 'max_daily_loss' in engine.risk_thresholds
        print("   âœ… Risk thresholds configured")

        # Test 2: Alert generation (simulate large loss)
        print("\n2ï¸âƒ£ Testing Alert Generation...")
        large_loss_fill = create_mock_fill("LOSS_001", "ORDER_003", "10.0", "40000.00", "sell", fee="50.00")
        await engine.process_fill(large_loss_fill)

        # Check for alerts
        recent_alerts = [alert for alert in engine.alerts if alert.timestamp > datetime.now() - timedelta(seconds=5)]
        print(f"   Generated {len(recent_alerts)} alerts")
        print("   âœ… Alert system functional")

        # Test 3: Alert handlers
        print("\n3ï¸âƒ£ Testing Alert Handlers...")
        alerts_received = []

        def test_alert_handler(alert):
            alerts_received.append(alert)

        engine.add_alert_handler(test_alert_handler)

        # Trigger another alert
        another_fill = create_mock_fill("ALERT_002", "ORDER_004", "5.0", "35000.00", "sell", fee="25.00")
        await engine.process_fill(another_fill)

        print(f"   Handler received {len(alerts_received)} alerts")
        print("   âœ… Alert handlers working")

        print("\nğŸ‰ RISK ALERT TESTS PASSED!")
        return True

    except Exception as e:
        print(f"\nâŒ Risk alert test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_performance_metrics(AnalyticsEngine):
    """Test performance metrics calculation."""
    print("\nğŸ“Š TESTING PERFORMANCE METRICS")
    print("=" * 60)

    try:
        engine = AnalyticsEngine("MetricsTestEngine")

        # Test 1: Sharpe ratio calculation
        print("\n1ï¸âƒ£ Testing Sharpe Ratio Calculation...")

        # Add some fills with varying returns
        test_fills = [
            create_mock_fill("M001", "O001", "1.0", "50000.00", "buy", "5.00"),
            create_mock_fill("M002", "O002", "1.0", "51000.00", "sell", "5.00"),  # Profit
            create_mock_fill("M003", "O003", "1.0", "50500.00", "buy", "5.00"),
            create_mock_fill("M004", "O004", "1.0", "49500.00", "sell", "5.00"),  # Loss
        ]

        for fill in test_fills:
            await engine.process_fill(fill)

        sharpe = engine.calculate_sharpe_ratio()
        print(f"   Calculated Sharpe ratio: {sharpe}")
        assert isinstance(sharpe, (int, float, type(None)))
        print("   âœ… Sharpe ratio calculation works")

        # Test 2: Profit factor
        print("\n2ï¸âƒ£ Testing Profit Factor...")
        profit_factor = engine.calculate_profit_factor()
        print(f"   Calculated profit factor: {profit_factor}")
        assert isinstance(profit_factor, (int, float, type(None)))
        print("   âœ… Profit factor calculation works")

        # Test 3: Performance benchmarks
        print("\n3ï¸âƒ£ Testing Performance Benchmarks...")
        benchmarks = engine.get_performance_vs_benchmarks()

        assert 'sharpe_vs_target' in benchmarks
        assert 'profit_factor_vs_target' in benchmarks
        print("   âœ… Performance benchmarks working")

        print("\nğŸ‰ PERFORMANCE METRICS TESTS PASSED!")
        return True

    except Exception as e:
        print(f"\nâŒ Performance metrics test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_integration_capabilities(AnalyticsEngine):
    """Test integration with other components."""
    print("\nğŸ”— TESTING INTEGRATION CAPABILITIES")
    print("=" * 60)

    try:
        engine = AnalyticsEngine("IntegrationTestEngine")

        # Test 1: System health
        print("\n1ï¸âƒ£ Testing System Health...")
        health = engine.get_system_health()

        assert 'status' in health
        assert 'session_uptime' in health
        assert 'data_points' in health
        assert health['status'] == 'active'
        print("   âœ… System health reporting works")

        # Test 2: Reset functionality
        print("\n2ï¸âƒ£ Testing Reset Functionality...")
        # Add some data first
        fill = create_mock_fill("RESET_001", "ORDER_RESET", "1.0", "50000.00", "buy", "5.00")
        await engine.process_fill(fill)

        assert len(engine.fill_history) == 1

        # Reset
        engine.reset_session_metrics()

        assert len(engine.fill_history) == 0
        assert engine.pnl.total_trades == 0
        print("   âœ… Reset functionality works")

        # Test 3: Configuration validation
        print("\n3ï¸âƒ£ Testing Configuration...")
        config = {
            'real_time_alerts_enabled': engine.enable_real_time_alerts,
            'performance_tracking_enabled': engine.enable_performance_tracking,
            'risk_thresholds_count': len(engine.risk_thresholds),
            'benchmarks_count': len(engine.benchmarks),
        }

        assert config['real_time_alerts_enabled'] == True
        assert config['performance_tracking_enabled'] == True
        assert config['risk_thresholds_count'] > 0
        assert config['benchmarks_count'] > 0
        print("   âœ… Configuration validation works")

        print("\nğŸ‰ INTEGRATION TESTS PASSED!")
        return True

    except Exception as e:
        print(f"\nâŒ Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Run all validation tests for Task 3.3.B.2."""
    print("ğŸ¯ TASK 3.3.B.2: REAL-TIME ANALYTICS ENGINE VALIDATION")
    print("=" * 70)
    print("Validating Real-time Fill Analytics Engine implementation")
    print()

    test_results = {}

    # Test imports
    import_success, AnalyticsEngine = test_imports()
    if not import_success:
        print("âŒ Cannot proceed - import failures")
        return False

    # Run test suites
    test_results['basic_functionality'] = await test_basic_functionality(AnalyticsEngine)
    test_results['risk_alert_system'] = await test_risk_alert_system(AnalyticsEngine)
    test_results['performance_metrics'] = await test_performance_metrics(AnalyticsEngine)
    test_results['integration'] = await test_integration_capabilities(AnalyticsEngine)

    # Generate report
    print("\n" + "=" * 70)
    print("ğŸ“Š TASK 3.3.B.2 VALIDATION RESULTS")
    print("=" * 70)

    total_tests = len(test_results)
    passed_tests = sum(1 for result in test_results.values() if result)

    print(f"ğŸ¯ Overall Result: {passed_tests}/{total_tests} test suites passed")
    print()

    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        description = test_name.replace('_', ' ').title()
        print(f"  {status} - {description}")

    print()

    if passed_tests == total_tests:
        print("ğŸ‰ ALL VALIDATION TESTS PASSED!")
        print("âœ… Task 3.3.B.2: Real-time Fill Analytics Engine - VERIFIED!")
        print()
        print("ğŸš€ Verified Features:")
        print("   â€¢ Real-time PnL tracking and calculation")
        print("   â€¢ Risk alert system with configurable thresholds")
        print("   â€¢ Performance metrics (Sharpe ratio, profit factor)")
        print("   â€¢ Execution quality monitoring")
        print("   â€¢ Live dashboard and reporting")
        print("   â€¢ Integration capabilities")
        print()
        print("ğŸ¯ READY FOR TASK 3.3.B.3: Advanced Fill Event System")
    else:
        print("âš ï¸ Some validation tests failed - additional work may be needed")

    print("=" * 70)
    return passed_tests == total_tests

if __name__ == "__main__":
    try:
        result = asyncio.run(main())
        sys.exit(0 if result else 1)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Validation interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
